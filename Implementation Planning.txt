# Football Data Implementation Plan

As your tech lead, here's a structured approach to complete your project, focusing on the specific data needed for your prediction model:

## Phase 1: API Endpoints & Data Requirements

First, let's identify the exact Sportmonks API endpoints needed for your prediction model:

1. **Core Endpoints**:
    
    - `/v3/football/leagues` - Already implemented
    - `/v3/football/seasons` - For season information
    - `/v3/football/fixtures` - Core match data
    - `/v3/football/fixtures/{id}` - Detailed fixture information
2. **Specialized Data Endpoints**:
    
    - `/v3/football/fixtures/{id}?include=odds` - Pre-match and in-play odds
    - `/v3/football/fixtures/{id}?include=statistics` - Team statistics (shots, possession)
    - `/v3/football/fixtures/{id}?include=events` - Timeline of match events
    - `/v3/football/livescores/inplay` - For collecting real-time data
3. **Required Include Parameters**:
    
    - `participants` - Team information
    - `scores` - Match scores
    - `statistics.periods` - Stats broken down by period (first half, full match)
    - `events.coordinates` - For ball position data
    - `odds.bookmaker.markets` - For pre-match and in-play odds

## Phase 2: Implementation Steps

### Step 1: Core Modules Setup (2-3 days)
1.  Refine `src/api/client.py` and `src.api.endpoints.py` (`EndpointHandler`).
2.  Set up `src/config.py`.
3.  Implement core database connection and table creation logic in `src/data/storage.py`.
4.  Define initial database schema in `Database planning.txt` and potentially a setup script or within `storage.py`. [cite: 11, 15, 16, 17, 19, 21, 23]

### Step 2: Implement Sync Scripts (Iterative, 1-2 days per resource)
For each required resource (Leagues, Seasons, Teams, Fixtures, Odds, Stats, etc.):
1.  **Create Processing Function:** Add a specific `process_<resource>_data` function in `src/data/processors.py` to transform the raw API JSON for that resource into a list of dictionaries matching the database schema.
2.  **Create Sync Script:** Create `scripts/sync_<resource>.py` (e.g., `sync_leagues.py`, `sync_seasons.py`).
    * This script will use `EndpointHandler` to fetch all data for the resource's endpoint.
    * It will call the corresponding `process_<resource>_data` function.
    * It will call database functions from `src/data/storage.py` (like `create_<resource>_table` if needed, and `store_data`) to save the processed data into the correct SQLite table.
3.  **Refine Database Schema/Storage:** Update `src/data/storage.py` with table creation logic for the new resource and adjust `store_data` if specific handling is needed. Update `Database planning.txt`.

### Step 3: Implement Supporting Logic (Ongoing)
1.  **Error Handling:** Enhance error handling within sync scripts, processors, and storage functions.
2.  **Logging:** Add proper logging throughout the process.
3.  **Incremental Updates:** (Advanced) Modify sync scripts and storage logic to handle fetching/storing only new or updated records based on timestamps or API parameters, if feasible.
4.  **Scheduling/Orchestration:** (Optional) Use OS tools (cron) or Python libraries (schedule) to run sync scripts automatically.

### Step 4: Testing & Validation (Ongoing)
1.  Test each sync script individually.
2.  Verify data integrity in the SQLite database after each sync.
3.  Run sample queries (like those in `Database planning.txt`) to ensure data is suitable for the prediction model.

## Immediate Next Steps
1.  Refactor `sync_leagues.py` as described above, moving logic to `src/data/processors.py` and `src/data/storage.py`.
2.  Update `README.md` to reflect this sync script pattern.
3.  Begin implementing the sync script for the next required resource (e.g., `sync_seasons.py`).
